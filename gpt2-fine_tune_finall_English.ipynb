{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nasser\\.conda\\envs\\finall\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df = pd.read_csv('merged_data.csv')\n",
    "df = df.sample(20000)\n",
    "lines = df['Verse'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the lines\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_length = max([len(tokenizer.encode(line)) for line in lines])\n",
    "tokenized_lines = tokenizer(\n",
    "    lines,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length+1,\n",
    "    add_special_tokens=True\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"but as a horseman checks the courser's speed\",\n",
       " 'and prides no longer in his beauteous form']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4360,\n",
       " 355,\n",
       " 257,\n",
       " 8223,\n",
       " 805,\n",
       " 8794,\n",
       " 262,\n",
       " 1093,\n",
       " 2655,\n",
       " 338,\n",
       " 2866,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_lines[4].index(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate input and label sequences\n",
    "input_sequences = [line[:-1] for line in tokenized_lines]\n",
    "labels = [line[1:] for line in tokenized_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4360,\n",
       " 355,\n",
       " 257,\n",
       " 8223,\n",
       " 805,\n",
       " 8794,\n",
       " 262,\n",
       " 1093,\n",
       " 2655,\n",
       " 338,\n",
       " 2866,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[355,\n",
       " 257,\n",
       " 8223,\n",
       " 805,\n",
       " 8794,\n",
       " 262,\n",
       " 1093,\n",
       " 2655,\n",
       " 338,\n",
       " 2866,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = tf.data.Dataset.from_tensor_slices(input_sequences)\n",
    "labels_dataset = tf.data.Dataset.from_tensor_slices(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine input and label datasets\n",
    "dataset = tf.data.Dataset.zip((input_dataset, labels_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch the dataset\n",
    "batch_size = 128\n",
    "dataset = dataset.shuffle(buffer_size=len(input_sequences))\n",
    "dataset = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "157/157 [==============================] - 122s 686ms/step - loss: 3.8992\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 113s 722ms/step - loss: 3.2953\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 117s 746ms/step - loss: 3.1904\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 115s 734ms/step - loss: 3.1236\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 116s 736ms/step - loss: 3.0730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16561080908>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "model.fit(dataset, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1: happy the birth of my redest lovewhat a matchless wind at their distant war\n",
      "Output 2: happy every godly precept all the same lyreto whom all gladsome pleasure and paintoward that solemn solemn day\n",
      "Output 3: happy britannia slain men his wonted herdson good the wise should say\n",
      "Output 4: happy envy rewards on who oh no lie his yoke giveshe meets what haste and madness slumber\n",
      "Output 5: happy a soul with no means for readingo and behold a shining light shine\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "seed_test = \"happy\"\n",
    "input_ids = tokenizer.encode(seed_test, return_tensors='tf')\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    temperature=1,\n",
    "    num_return_sequences=5,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Print generated output\n",
    "for i, output in enumerate(sample_outputs):\n",
    "    print(f\"Output {i + 1}: {tokenizer.decode(output, skip_special_tokens=True)}\")\n",
    "#  \"chill'd the fair dawning of the yearin honour of th' auspicious morn\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2_tokenizer_fine_tuned_model_english\\\\tokenizer_config.json',\n",
       " 'gpt2_tokenizer_fine_tuned_model_english\\\\special_tokens_map.json',\n",
       " 'gpt2_tokenizer_fine_tuned_model_english\\\\vocab.json',\n",
       " 'gpt2_tokenizer_fine_tuned_model_english\\\\merges.txt',\n",
       " 'gpt2_tokenizer_fine_tuned_model_english\\\\added_tokens.json',\n",
       " 'gpt2_tokenizer_fine_tuned_model_english\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "model.save_pretrained('gpt2_fine_tuned_model_english')\n",
    "# save tokenizer\n",
    "tokenizer.save_pretrained('gpt2_tokenizer_fine_tuned_model_english')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
