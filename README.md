# Report: Poem Generation Models

## Poetry Generation Models

**Author:** Eleen Kmail  
**Course:** NLP Course  
**Instructor:** Ms. Yamama Shakaa  

## Introduction

Poetry generation is a fascinating field within Natural Language Processing (NLP) that aims to generate artistic and expressive poems using computational models. In this report, we will discuss the process and techniques used to train and evaluate two poetry generation models, one for Arabic poetry and the other for English poetry.

## Data Collection

All the poetries are collected from pre-collected datasets already published on the internet. For this project, I did not collect them by myself.

- **Arabic Poetry Dataset:** I utilized the "Arabic Poetry Dataset (6th - 21st century)" available on Kaggle. This dataset contains over 58K Arabic poems from different time periods, providing a rich source for training our model. [Kaggle: Arabic Poetry Dataset](https://www.kaggle.com/datasets/fahd09/arabic-poetry-dataset-478-2017)
  
- **English Poetry Dataset:** We used the "Poem Dataset" provided by HCI Lab, which offers a diverse collection of English poems. It consists of 199,002 verses, each labeled with one of these four meters: Iambic, Trochee, Dactyl, and Anapaestic. [HCI Lab: Poem Dataset](https://hci-lab.github.io/LearningMetersPoems/)

## Models

Both Arabic and English models are based on the GPT2 model:
- **Arabic Model:** I fine-tuned the Arabic version of GPT2, called aragpt2-base, which is made specifically for Arabic text. [Hugging Face: Arabic Text Generator (aragpt2-base)](https://huggingface.co/aubmindlab/aragpt2-base)
- **English Model:** I fine-tuned the GPT2-base model without pre-training on poems before. [Hugging Face: English Text Generator based on GPT2](https://huggingface.co/gpt2)

## Embedding

In generating poetry in both Arabic and English, I chose not to rely on transformer-based embeddings. Instead, I utilized sequence representation techniques for encoding the texts, converting the text into vector integer indexes assigned by a GPT2 tokenizer. The input is the whole sequence excluding the final word, and the label is the whole sequence excluding the first word.

## Libraries Used

- **transformer:** Import models from Hugging Face.
- **SentenceTransformer:** Import models capable of encoding sentences into vector representations.
- **cos_sim:** Measure the similarity between two sentence embeddings.
- **pyarabic.araby:** Remove diacritics from Arabic text.
- **tensorflow.keras.preprocessing.sequence:** Preprocess sequences for model training and generation.

## Model Evaluation

For evaluating our poetry generation models, we employed Semantic Textual Similarity (STS) with the Sentence-BERT pre-trained model. By embedding both the ground truth sentences (actual poems) and the predicted sentences generated by our models using Sentence-BERT, we computed the cosine similarity between the embeddings to assess the quality and coherence of the generated poems.

## Comparative Between Arabic and English Models
![Arabic Model Evaluation](images/Arabic_model_evaluation.png)

![English Model Evaluation](images/English_model_evaluation.png)

The cosine similarity score between the input and the predicted text (after embedding them using the S-BERT model) indicates that the Arabic model has better performance than the English model for the majority of the samples.

## Human Interpretability Evaluation

### Arabic Model:
![Arabic Model](images/Arabic_model_results.png)
- **Seed Text:** الحزن
- **Generated Output:** The model generates lines related to the topic, connecting sadness with the heart. However, the lines lack rhyme.

### English Model:
![English Model](images/English_model_results.png)
- **Seed Text:** happy
- **Generated Output:** The model produces words like shine, love, good, and pleasure, all related to the seed text. It connects "happy" with "soul" in a meaningful context.

## Suggestions for Performance Improvements

We researched methods to enhance model performance and prevent overfitting. One approach is to use a new loss function during training that compares the similarities between the embeddings for the ground truth (actual) and predicted labels, rather than comparing the actual and predicted values directly.

## Models Deployment on Hugging Face Platform

- **Arabic Model:** [Hugging Face: ArabicModelGenerator](https://huggingface.co/EleenKmail/ArabicModelGenerator)
- **English Model:** [Hugging Face: EnglishModelGenerator](https://huggingface.co/EleenKmail/EnglishModelGenerator)
